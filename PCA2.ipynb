{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40429950-0424-4ebb-b8a6-1438773a049e",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "### What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169bfe7-b6d0-47fb-96a1-4b6bac907d9f",
   "metadata": {},
   "source": [
    "- Projection in the context of PCA refers to the transformation of data points onto a new subspace defined by principal components. \n",
    "\n",
    "- In PCA, projections are used to reduce the dimensionality of the data while retaining as much variance as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f3942-3df3-4a68-b23e-ae1b51ddf877",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "### How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e8a48-d295-4e4f-8275-0efbd4511fb2",
   "metadata": {},
   "source": [
    "- The optimization problem in PCA aims to find the directions that maximize the variance of the projected data.                          \n",
    "\n",
    "- the optimization problem involves finding eigenvectors of the covariance matrix of the data. The eigenvectors corresponding to the largest eigenvalues are the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364170de-c7ff-40ae-abb1-657d5639dd53",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "### What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadf9b0-3c50-4ced-ae2c-817fb7dc8e97",
   "metadata": {},
   "source": [
    "The covariance matrix captures the covariance between pairs of features in the data.\n",
    "\n",
    "- Eigenvalues and Eigenvectors: PCA involves computing the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues represent the magnitude of variance in these directions.\n",
    "- Principal Components: The eigenvectors of the covariance matrix are the principal components, and they are orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e669140-bea4-457f-9352-4f8fb40261de",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "### How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1830d-b84b-4bb3-9523-f00445bcb50a",
   "metadata": {},
   "source": [
    "- Variance Retained\n",
    "- Dimensionality Reduction\n",
    "- Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef01581-d9db-4ae3-b39d-736b38509fdf",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "### How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7f069-16d0-443c-a2fe-57e61957d019",
   "metadata": {},
   "source": [
    "- PCA can be used in feature selection by identifying the principal components that capture the most significant features of the data. \n",
    "\n",
    "- Dimensionality Reduction\n",
    "- Noise Reduction\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03149b4d-7fbc-46c4-951d-1498ffba0462",
   "metadata": {},
   "source": [
    "# Q6\n",
    "### What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111e468-cc5f-453e-a656-05b2895b6dd3",
   "metadata": {},
   "source": [
    "- Data Preprocessing\n",
    "- Image Compression\n",
    "- Data Visualization\n",
    "- Noise Filtering\n",
    "- Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd77ad-c43f-4dc5-b224-b998dfe59b95",
   "metadata": {},
   "source": [
    "# Q7\n",
    "### What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c08c1-13de-4db1-b283-415bf88a9915",
   "metadata": {},
   "source": [
    "- spread refers to the dispersion of data points along different directions. Variance is a measure of this spread. Principal components are the directions along which the spread (variance) of the data is maximized. Thus, PCA identifies the directions (principal components) with the largest variance (spread) in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a26bf-5692-4a9b-8524-e8e974de480b",
   "metadata": {},
   "source": [
    "# Q8\n",
    "### How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71d634-38b3-44c3-9a1b-96a084d0eca6",
   "metadata": {},
   "source": [
    "- Calculating the Covariance Matrix\n",
    "- Finding Eigenvectors and Eigenvalues\n",
    "- Sorting and Selecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41b44c-b3af-40c1-b0dd-27308a4f2137",
   "metadata": {},
   "source": [
    "# Q9\n",
    "### How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632b455-56a9-4bdf-ae20-ac644164037b",
   "metadata": {},
   "source": [
    "- By Prioritizing High Variance Dimensions\n",
    "- Dimensionality Reduction\n",
    "\n",
    "- By focusing on dimensions with high variance, PCA ensures that the most significant patterns in the data are retained while reducing the complexity and noise from less informative dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
